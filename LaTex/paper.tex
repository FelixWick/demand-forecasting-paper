\documentclass[BCOR=1mm, DIV=calc,10pt,
twoside=true,
twocolumn,
headings=normal]{scrartcl}
%\KOMAoptions{DIV=calc}% recalculate the page layout with a calculated DIV value

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{amsmath}
\usepackage{bm}
\usepackage{url}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{authblk}


\newcommand{\fig}{Fig. }
\newcommand{\eqn}{Eqn. }
\newcommand{\tab}{Tab. }
\newcommand{\wrt}{w.r.t. }
\newcommand{\etal}{ {\em et al. }}


\begin{document}


\title{Demand Forecasting of individual Probability Density Functions with Machine Learning}

\author[1]{Felix Wick \thanks{felix.wick@blueyonder.com}}
\author[2]{Ulrich Kerzel \thanks{u.kerzel@iubh-fernstudium.de}}
\author[1]{Trapti Singhal \thanks{trapti.singhal@blueyonder.com}}
\author[1]{Martin Hahn \thanks{martin.hahn@blueyonder.com}}
\affil[1]{\small Blue Yonder GmbH (Karlsruhe, Germany)}
\affil[2]{\small IUBH Internationale Hochschule (Erfurt, Germany)}


\date{}

\maketitle

\begin{abstract}
Demand forecasting is a central component for many aspects of supply chain operations as it provides crucial input for subsequent decision making like ordering processes. While machine learning methods can significantly improve prediction accuracy over traditional time series forecasting, the calculated predictions are often mere point estimations for the conditional mean of the underlying probability distribution, and the most powerful approaches, like deep learning, are usually opaque in terms of how its individual predictions can be interpreted. Using the novel supervised machine learning method ``Cyclic Boosting'', complete individual probability density functions in form of negative binomial distributions can be predicted instead of simple point estimates. While metrics evaluating simple point estimates are widely used, methods for assessing the accuracy of predicted distributions are rare and this work proposes new techniques for both qualitative and quantitative evaluation methods. Additionally, each single prediction obtained with this framework is explainable, which is a major benefit in practice as individual forecasts can be understood by the practitioner and ``black-box'' models can be avoided. 
\end{abstract}

{Keywords: \textbf{explainable machine learning, demand forecasting, probability distribution}}


\section{Introduction}

Demand forecasting is one of the main challenges for retailers and at the core of business operations. Due to its stochastic nature, demand is difficult to forecast as it depends on many influencing factors and the realized demand can be interpreted as a random variable that is described by an appropriate probability density function (PDF). In order to make operational decisions, an optimal point estimator has to be defined, that can be used to derive ordering decisions used in the replenishment process of the retailer. Demand estimation is further complicated by the fact that retailers typically only observe realized sales rather than the actual demand, and in case the demand exceeds the current stock level the data become censored. The ordering decision process is complicated by a range of factors: Even in the case of perfect demand forecasts, the decision maker has to consider lot-sizes defined by the wholesaler or manufacturer as well as to balance conflicting metrics to reach an optimal decision: Ordering too few items may result in stock-out situations leading to unrealized demand and unsatisfied customers. Ordering too many items results in excess inventory which increases transport and storage costs and, in the case of perishable goods, excessive waste, as spoilt items need to be disposed of at additional cost and potentially even environmental impact. This situation is particularly noticeable in the so called "ultra-fresh'' category, which includes items such as bakery products, ready-meals, fresh diary products, or certain meat products such as ground meat. These items typically have a shelf-life ranging from less than a business day to a few business days at most, with a continuous spectrum in between, depending on the exact item. In many situations, additional constraints have to be considered to reach an optimal ordering decision: Delivery cycles of items may vary depending on the type of item and the wholesaler or manufacturer from which they are procured. Retailers also operate at a given service level to guarantee that a certain level of demand can be fulfilled. The exact service level typically depends on the overall business policy of the retailer and may also depend on individual products, ranging from "never-out-of-stock'' items to a service level exceeding e.g. 90\%.

The availability of Big Data allows capturing, storing and processing a vast amount of data associated with demand, such as historic sales records, information about promotional events or advertisements, pricing information, local weather at retail locations, seasonal information as well as a wide range of further variables. Modern machine learning algorithms can then be used to predict the per-item demand distribution, corrected for censored data, from which an optimal point estimator can be derived to be used in the subsequent ordering decision. It is important to note that demand as a random variable is not identically and independently distributed (i.i.d.). While the probability distribution describing the demand can be attributed to a given family or parameterization, the exact parameters vary: Seasonal effects, finite life cycles of products and the introduction of new products influence the demand distribution, as well as the local weather at the retail location or the retail location itself in terms of size, assortment range, customer diversity and other factors. The retailers themselves also actively influence demand by using advertisements to highlight products, offering rebates or discounts for specific products as well as pursuing an active pricing strategy. This means that while we can generally assume that demand follows a specific type of probability distribution, its parameters are unique to the  instance for which an estimate is required. For example, the probability distribution governing the demand of a particular item is specific to the item, date and retail location for which the forecast is made and depends on a wide range of further influencing factors.

The remainder of the paper is organized as follows: We first review the relevant literature and existing work in sec. \ref{sec:LitRev}. We then describe our method to predict individual negative binomial PDFs by means of a parametric approach including two distinct machine learning models for mean of variance in sec. \ref{sec:pdfEstimation}. After that we describe methods for the qualitative and quantitative evaluation of PDF predictions in sec. \ref{sec:pdfEvaluation}. And finally, we present a demand forecasting example to show an application of our methods in sec. \ref{sec:example}.


\section{Literature Review}
\label{sec:LitRev}

Inventory management offers a rich theory and the extensive body of research can be broadly grouped into two categories, where the inventory control problem is either based on some knowledge of the underlying demand  distribution or an integrated approach that seeks to map directly from the available data (historic sales records and further variables) to the ordering decision. This approach is often referred to as "data-driven newsvendor'' and discussed  e.g. in \cite{beutel2012safety,ban2019big,bertsimas2020predictive, oroojlooyjadid2020applying}. It aims to avoid estimating the underlying probability distribution for demand and use the available data to derive the operational decisions (the order quantity) directly.  An overview of a range approaches can also be found in \cite{huber2019data}.

Although this approach seems preferable at first glance, since it avoids determining the full demand distribution and results directly in the desired operational decision (the order quantity), it faces several drawbacks. First, the full probability distribution for the demand of a specific item at a given sales location and business day includes all available information including the uncertainty of the modelled demand. This can be used to simulate the performance on a per-item level and e.g. optimize the impact on business strategy decisions on conflicting metrics such as stock out- and waste-rate. By forecasting the full demand distribution as opposed to point estimators such as the expected demand or the median of the distribution, the forecast quality can be evaluated for quantiles of the distribution, including the often extensive tails of the distribution. Additionally, a purely data-driven approach going from the observed data directly to the operational decision (such as the order quantity) does not allow to analyze the data-generating process, i.e. the mechanism behind the stochastic behavior of the customer demand. However, modelling demand directly is vital if a causal analysis is planned at a later stage or independently, for example to study the effect promotions, advertisements,  price changes or other influencing factors in either Pearl's do-calculus \cite{PearlCausality} or Rubin's potential outcomes framework \cite{rubin1974estimating}. Using an operational quantity such as the order quantity will in most cases act as an insufficient proxy of the quantity of interest (customer demand) and likely lead to unnecessary causal pathways that may not be able to be fully controlled for. From a practical perspective, separating the demand forecast from the operational decisions (i.e. calculating the  order quantities for the next delivery cycle) also allows to evaluate longer-term planning and reduces the complexity as it avoids coupling the complex delivery schedules of multiple wholesalers and manufacturers with the forecast of customer demand. This also allows to share long-term demand predictions with other business units or external vendors  and wholesalers to ease their planning for the production and supply-chain processes upstream of the retailer. From the perspective of industrial practice, modelling the demand separately from deriving the subsequent orders  has the additional benefit that multiple retail chains can benefit from any improvement in the model description even if the concrete retailers are unrelated to each other. For example, if a particular effect was identified at some retailer A and included in the machine learning model, the improvement can be rolled out immediately or on request to all other retailers using this system on a planetary scale without having to adapt the underlying machine learning model for each retailer individually.

In contrast to the direct mapping the observed data to ordering decisions, more traditional inventory control systems rely on the knowledge of the demand distribution in one form or another, see e.g. \cite{silver1998} for an overview. In $(s,S)$ type inventory control systems \cite{Scarf1958}, inventory levels are monitored at regular intervals and orders are dispatched once the inventory level reaches a minimal value $s$. In case of linear holding and shortage costs, such policies are optimal \cite{Scarf1959}, although perishable goods pose more challenges, see e.g. \cite{Nahmias1973,Nahmias1975,nahmias1978}. Additionally, service level constraints can be included in these kind of inventory control systems \cite{minner2010periodic}. Perishable goods are well described by the "newsvendor-problem'' \cite{Edgeworth}, where in the simplest case all stock perishes at the end of the selling period (e.g. a business day). For a detailed review of the newsvendor problem see e.g. \cite{Khouja1999537}. Assuming linear underage and overage costs $b,h >0$, the optimal quantile $q_{\mathrm{opt}} = {b}/{(b+h})$ of a known demand distribution $f(D)$ can be calculated exactly.

The main objective in any of these approaches is to determine the underlying demand distribution. The simplest approach is to just use the observed sales events and forecast these as a time series (see e.g. \cite{alwan2016}) or via sample average approximation (SAA), see e.g. \cite{shapiro2014} for an overview. However, these approaches do not make use of any data apart from the sales record themselves, although we know that many variables such as price or advertisements influence, and therefore are highly correlated with, the demand. Saghafian and Tomlin \cite{saghafian2016newsvendor} propose to include partial information about the distribution in the derivation of the operational decision, i.e. the  calculation of the optimal order quantity.

However, in order to be able to fully optimize the operational decision, it is critical that one indeed reconstructs a full demand distribution. This also implies that a simple point-estimator, as provided by the most common statistical techniques and machine-learning approaches, will not suffice. Additionally, we need to consider that demand is not i.i.d., but depends on external factors such as season, weather, product life-cycle, and is also actively changed by the retailer by setting a specific price, offering rebates or running advertisements. Additionally, the demand implicitly depends on the location of the retail outlet as well as the specifics of that location, such as product  assortment influencing the choice of possible replacement articles and many more. These complications are the main reason we cannot treat the replenishment process as $n$ independent newsvendor-type problems.

Instead, we need to determine the full demand distribution from data, conditional on the relevant variables such as date, location, and item, taking all auxiliary data such as article characteristics, pricing, advertisements, retail location details, etc. into account. This can be done in several ways: Quantile regression \cite{koenker2001, wen2017} can be implemented in various frameworks and used to estimate a range of quantiles for each predicted distribution from which an empirical probability distribution can be interpolated. Using a dedicated neural network \cite{Feindt2006190}, either the  full probability distribution or a defined range of quantiles can be calculated directly from the data for each individual prediction without assuming an underlying model. Alternatively, one can assume a given demand model and fit the model parameters instead of reconstructing the complete distribution \cite{astonpr373, SALINAS20201181}. This approach is computationally favourable, as fewer parameters need to be estimated compared to the case of the full distribution. Empirically, one can determine the best fitting distribution from data \cite{adan1995}. However, given the stochastic nature of the demand, such an empirically determined distribution is not expected to be stable and prone to sudden changes. Instead, the choice of the demand distribution should be motivated by theoretic considerations. The discrete demand is typically modelled as a negative binomial distribution (NBD), also known as Gamma-Poisson distribution \cite{Ehrenberg1959,Ehrenberg1967,Ehrenberg1972,Chatfield1973,Schmittlein_1985}. This distribution arises if the Poisson parameter $\mu$ is a random variable itself that follows a Gamma distribution. The NBD has two parameters, $\mu$ and $ \sigma^2 > \mu$, and is over-dispersed compared to the Poisson distribution for which $\mu = \sigma^2$. Hence, for each ordering decision, the model parameters $\mu$ and $\sigma$ need to be determined for each item at the required granularity, typically for each sales location and ordering time, depending on all auxiliary data describing article details, retail location, and influencing factors such as pricing and advertisement information.

\subsection*{Summary of Contributions}

This work demonstrates how the explainable machine learning algorithm Cyclic Boosting \cite{Wick2019} can be used to model the demand distribution at the granularity needed by the retailer. Typically this means that the full demand distribution has to be estimated per SKU for each sales location and opening day, conditional on a wide range of variables such as weather, prices, promotions, etc. In contrast to using a "black-box'' machine learning model, using Cyclic Boosting allows to interpret how each individual prediction was made and what the most important variables are per individual prediction.

Additionally, we show how the cumulative distribution function (CDF) can be used to accurately asses the forecast quality of the full predicted demand distribution, including the tails of the distribution. This allows to verify that the predicted demand distribution accurately reflects the observed data and can hence be used both to derive operational decisions such as  order quantities as well as strategic business decisions by the retailer or gain further insights into customer behavior using for example causal modelling.


\section{Negative Binomial PDF Estimation}
\label{sec:pdfEstimation}

To predict an individual PDF using a parametric approach, one has to rely on a model assumption about the underlying distribution of the random variable to be predicted. As discussed earlier, the negative binomial probability distribution (NBD) is well routed in theoretical arguments to model customer demand. Its parameters can be modelled by two independent models, one to estimate the mean and the other for the variance. At least in principle, any method can be used. However, in the case of demand forecasting, each prediction is highly specific to the circumstances in which it is used (such as SKU, opening day, and store location) and may depend on a multitude of describing variables or features such as sales location, weather, price, and so forth. It should be noted that these parameters are not independent between products. For example, a promotion applied to one product can, at least in principle, affect the sales of related products within the assortment. This implies that the demand forecasts cannot be treated as individual newsvendor-type predictions but need to be modelled holistically. Machine learning algorithms are ideally suited for this task and in the following we will use the Cyclic Boosting algorithm to benefit in particular from explainable decisions rather than black-box approaches. Furthermore, the regularization approach used during training of the Cyclic Boosting algorithm allows a dedicated treatment of the underlying NBD model, which is another major benefit compared to a standard ''off-the-shelf'' machine learning algorithm. This means we use two subsequent Cyclic Boosting models in order to estimate the parameters of each individual PDF that we need to forecast, where the first model is used to estimate the mean and the second to estimate the variance. The features may or may not differ between the mean and variance estimation models. The assigned mean and variance predictions can then be used to generate individual PDFs using the parameterization of the NBD for each sample.

In the following, after a brief recap of the fundamental ideas of Cyclic Boosting, we describe a method to predict mean and variance for individual NBD models using Cyclic Boosting.

\subsection{Cyclic Boosting Algorithm: Mean estimation}
\label{sec:CB}

Cyclic Boosting \cite{Wick2019} is a type of generalized additive models using a cyclic coordinate descent optimization and featuring a boosting-like update of parameters. Major benefits of Cyclic Boosting are its accuracy, performance even at large scale and providing fully explainable predictions which are of vital importance in practical applications, in particular for multi-national retail vendors.

The main idea of this algorithm is the following: First, each feature, denoted by index $j$, is discretized appropriately into $k$ bins to reflect the specific behaviour of the feature. The global mean $\mu$ is determined from all target values $y$ of the  target variable $Y \in [0,\infty)$ observed in the data. Single data records, for example the sales of a specific SKU along with all relevant features, are indexed by $i$.
The individual predictions $\hat{y_i}$  can then be calculated as:
\begin{equation} \label{eqn:cb}
\hat{y}_i = \mu \cdot \prod \limits_{j=1}^p f^k_j \quad \text{with}\; k=\{ x_{j,i} \in b^k_j\}
\end{equation}
The factors $f^k_j$ are the model parameters that are determined iteratively from the features, and are determined iteratively until the algorithm converges. During training,  regularization techniques are applied to avoid overfitting and improve the generalization ability of the algorithm. The deviation of each factor from $f^k_j=1$ can then be used to explain how a  specific feature contributes to each individual prediction.

In detail, the following meta-algorithm describes how the model parameters $f^k_j$ are obtained from the training data:
\begin{enumerate}
\item{Calculate the global average $\mu$ from all observed $y$ across all bins $k$ and features $j$.}
\item{Initialize the factors $f^k_j \leftarrow 1$}
\item{Cyclically iterate through features $j = 1,...,p $ and calculate in turn for each bin $k$ the partial factors $g$ and corresponding aggregated factors $f$, where indices $t$ (current iteration) and $\tau$ (current or preceding iteration) refer to iterations of full feature cycles as the training of the algorithm progresses:
\begin{equation} \label{factors}
g^k_{j,t} = \frac{\sum \limits_{x_{j,i} \in b^k_j} y_i}{\sum \limits_{x_{j,i} \in b^k_j} \hat{y}_{i,\tau}}\;\; \mathrm{where} \; \; f^k_{j,t} = \prod \limits_{s=1}^t g^k_{j,s}
\end{equation}
Here,  $g$ is a factor that is multiplied to $f_{t-1}$ in each iteration. The current prediction, $\hat{y}_\tau$, is calculated according to eqn. \ref{eqn:cb} with the current values of the aggregated factors $f$:
\begin{equation} \label{factors3}
\hat{y}_{i,\tau} = \mu \cdot \prod \limits_{j=1}^p f^k_{j,\tau}
\end{equation}
To be precise, the determination of $g^k_{j,t}$ for a specific feature $j$ uses $f^k_{j,t-1}$ in the calculation of $\hat{y}$. For the factors of all other features, the newest available values are used, i.e., depending on the sequence of features in the algorithm, either from the current ($\tau=t$) or the preceding iteration ($\tau=t-1$).}
\item{Quit when stopping criteria, e.g. the maximum number of iterations or no further improvement of an error metric such as the mean absolute deviation (MAD) or mean squared error (MSE), are met at the end of a full feature cycle.}
\end{enumerate}

\subsection{Cyclic Boosting Algorithm: Width estimation}
\label{sec:cb_width}

In the previous section, the general Cyclic Boosting algorithm was used to estimate the mean of the NBD model. In order to predict the variance of the NBD model (associated with the mean predicted before), we modify the algorithm like described in the following.

When looking at the demand of individual SKUs, the target variable $y$ has the values $y = 0, 1, 2, ...$ and the NBD model can be parameterized as in \cite{hilbe2011negative}:
\begin{equation} \label{eqn:nbinom}
\mathrm{NB}(y; \mu, r) = \frac{\Gamma(r + y)}{y! \cdot \Gamma(r)} \cdot \left(\frac{r}{r + \mu}\right)^r \cdot \left(\frac{\mu}{r + \mu}\right)^y,
\end{equation}
where $\mu$ is the mean of the distribution and $r$ a dispersion parameter.

By bounding the inverse of the dispersion parameter $1/r$ to the interval $[0, 1]$ (corresponding to bounding $r$ to the interval $[1, \infty]$), the variance $\sigma^2$ can be calculated from $\mu$ and $r$ via:
\begin{equation} \label{eqn:variance_r}
\sigma^2 = \mu + \frac{\mu^2}{r}
\end{equation}

The estimate of the dispersion parameter $\hat{r}$ can then be calculated by minimizing the loss function defined in \eqn \eqref{eqn:loss_likelihood}, which is expressed as negative log-likelihood function of a negative binomial distribution. Using Cyclic Boosting, the minimization over all input samples $i$ is performed with respect to the Cyclic Boosting parameters $f^k_j$, constituting the model of $\hat{r_i}$, according to \eqn \eqref{eqn:r}, where the estimates for the mean $\hat{\mu_i}$ are fixed to the values obtained in the previous step (described in sec. \ref{sec:CB}).

\begin{equation} \label{eqn:loss_likelihood}
L(r) = -\mathcal{L}(r) = -\ln \sum_i \mathrm{NB}(y_i; \hat{\mu_i}, \hat{r_i})
\end{equation}

\begin{equation} \label{eqn:r}
\hat{r_i} = 1 + \prod \limits_{j=1}^p f^k_j \quad \text{with}\; k=\{ x_{j,i} \in b^k_j\}
\end{equation}

In other words, the values $\hat{r_i}$ are estimated via learning the Cyclic Boosting model parameters $f^k_j$ for each feature $j$ and bin $k$ from data. For any concrete observation $i$, the index $k$ of the bin is determined by the value of the feature $x_{j,i}$ and the subsequent look-up into which bin this observation falls. Like in sec. \ref{sec:CB}, the model parameters $f^k_j$ correspond to factors with values in $[0, \infty]$ and again values deviating from $f^k_j=1$ can be used to explain the relative importance of a specific feature contributing to individual predictions. Note that the structure of \eqn \eqref{eqn:r} can be interpreted as inverse of a logit link function in the same way as explained in \cite{Wick2019} when Cyclic Boosting is used for classification tasks.

The Cyclic Boosting algorithm is trained iteratively using cyclic coordinate descent, processing one feature with all its bins at a time until convergence is reached. Unlike in the basic multiplicative regression mode of Cyclic Boosting described in sec. \ref{sec:CB}, the minimization of the loss function in \eqn \eqref{eqn:loss_likelihood} cannot be solved analytically and has to be done numerically, for example using a random search. All other advantages of Cyclic Boosting, like for example individual explainability of predictions, remain valid for its negative binomial width mode.

Finally, the variance $\hat{\sigma}^2_i$ can be estimated from the dispersion parameter $\hat{r_i}$ using \eqn \eqref{eqn:variance_r}. Using  the individual predicted mean $\hat{\mu_i}$ from the first step, the model is fully specified for each individual prediction $i$.


\section{Evaluation of PDF Predictions}
\label{sec:pdfEvaluation}

Many statistical and most machine learning methods do not provide a full probability density distribution as result. Instead, these methods typically predict a single numerical quantity that is then compared to the observed concrete realization of the random variable using metrics such as the mean squared error (MSE),  mean absolute deviation (MAD) or others. In the setting of a retailer, the observed quantity is the sales of individual products (denoted by $y$) and most machine learning approaches would then predict a single number $\hat{y}$ as a direct estimate of the sales. However, reducing the prediction to a single number does not allow to account for the uncertainty of the prediction or the dynamics of the system. Instead, it is imperative to predict the full PDF for each prediction to be able to optimize the subsequent operational decision. Unfortunately, most statistical or machine learning methods that predict full individual probability functions lack quantitative or at least qualitative evaluation methods to assess whether the full distribution has been forecast correctly, in particular in the tails of the distribution.

For an estimation of the determining parameters of an assumed functional form for the PDF, assessing the correctness of the PDF model output refers to the evaluation of the accuracy of the prediction of the different determining parameters. In the case of the negative binomial distribution used in this work, we have to verify that mean and variance are determined accurately, as well as checking that the choice of the underlying model can describe the observed data.

In the following, we will show how different visualizations of the observed cumulative distribution function (CDF) values can be used to evaluate the quality of the predicted PDFs. Although we limit the following discussion to the negative binomial model, the method can be applied generally to any representation of a probability density distribution, even if the PDF is obtained empirically.

\subsection{Qualitative Evaluation of PDF Predictions}

In the simplest case, we only have one model with one set of model parameters to cover all predictions. In this case, the evaluation of the full probability distribution is straight-forward: We would fill a histogram of all observed values, such as sales records, and overlay this with the single model, such as a negative binomial with predicted parameters, that is used for all observations. Then, we compare the model curve directly with the observations, using statistical tests such as the Kolmogorov-Smirnov test.

In practical applications however, we have a large number of effective prediction models, since although we always use the same model parameterization, such as the negative binomial distribution, its parameters have to be determined at the required level of granularity. For example, for daily orders, we need to predict the parameters of the negative binomial distribution for each location, sales day, and product. Unlike the simple case discussed above, where we had many observations to compare the prediction model to, we now have just a single observation per prediction, meaning that we cannot use statistical tests directly.

\subsubsection{Histogram of CDF Observations}
\label{sec:cdf_histo}

For a first qualitative assessment, we make use of the probability integral transform, see e.g. \cite{Angus1994,casella2002statistical}, which states that a random variable distributed according to the CDF of another random variable is uniformly distributed. So, in case of a correctly calibrated PDF prediction, the distribution of the corresponding actually observed CDF values of the individual PDF predictions is expected to be uniform, {\em regardless} of the shape of the predicted distribution, and any deviation can be interpreted as a hint that the predicted PDF is not fully correct \cite{diebold1998vevaluating}.

The CDF of a PDF $f(x)$ is defined as:

\begin{equation}
\label{eqn:CDF}
F_X(x) = P(X \le x) = \int_{-\infty}^{x} f_X(x^\prime) dx^\prime
\end{equation}

Here, $F_X(x)$ is the CDF with $\lim_{x \to -\infty}F_X(x) = 0$ and $\lim_{x \to \infty}F_x(x) = 1$. The cumulative distribution describes the probability that a the variable has a value smaller than $x$ and intuitively represents the area under $f(x^\prime)$ up to a point $x$.

If the CDF is continuous and strictly increasing, then the inverse of the CDF, $F^{-1}(y)$, exists and is a unique real-valued number $x$ for each $y \in [0,1]$, so that we can write $F(x) = y$. The latter defines the inverse quantile function, because we can define the quantile $\tau$ of the probability distribution $f(x)$ as:

\begin{equation}
Q_\tau = F^{-1}(\tau)
\end{equation}

Using the example of the normal distribution with $\mathcal{N}(0,1)$ as shown in \fig \ref{fig:PdfCdf}, we can identify the median ($\tau = 0.5$) by first looking at the CDF in the lower part of the figure, look at $y=0.5$ on the $y$-axis and then identify the point on the $x$ axis for both the PDF $f(x)$ and the CDF $F(x)$ that correspond to the quantile $\tau$. In the case of the normal distribution, this is of course the central value at zero.

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{../figures/PdfCdf}
\caption{\label{fig:PdfCdf} Probability distribution function and cumulative distribution function of a normal distribution.}
\end{center}
\end{figure}

We can then interpret the CDF as a new variable $s = F(t)$, meaning that $F$ becomes a transformation that maps $t$ to $s$, i.e. $F:t \to s$. Accordingly,  $\lim_{t \to -\infty}s(t) = 0$ and $\lim_{t \to \infty}s(t) = 1$ and $s$ can be intuitively interpreted as the fraction of the distribution of $t$ with values smaller than $t$ from the definition of the CDF. This implies that the probability distribution of $s$, $g(s)$, is constant in the interval $s \in [0,1]$ in which $s$ is defined, and $s$ can be interpreted as the cumulative distribution of its own probability distribution:

\begin{equation}
s = G(s) = \int_{-\infty}^{s} g(s^\prime) ds^\prime
\end{equation}

In case of discrete probability functions, such as the negative binomial function, the same argument still holds, but the definition of the inverse quantile function is replaced by the generalized inverse: $F^{-1}(y) = \mathrm{\inf \left \{x : F(x)>y\right  \} }$ for $y \in [0,1]$, see e.g. \cite[p. 54]{casella2002statistical}. In order to obtain a uniform distribution for discrete PDFs that is comparable to the case of continuous distributions, the histogram holding the inverse quantiles or values of the CDF is filled using random numbers according to the intervals of the CDF. For example, if the sales of zero items accounts for 75\% of the observed sales distribution for this item, the value of the CDF function that is used to fill the histogram is randomly chosen in the interval $[0,0.75]$. Proceeding similarly for all other observed values, the resulting histogram of CDF values should again be uniform as in the case of a continuous PDF.

A histogram of the observed quantiles (or CDF values) for each individual PDF prediction is therefore expected to be uniformly distributed in $[0,1]$, if the predicted probability distribution $f(x)$ is correctly calibrated. This is illustrated in \fig \ref{fig:cdf_histos}, which shows the distribution of observed quantiles for five different cases. If both the choice of the model and the model parameters are estimated correctly, we would expect the uniform distribution. If the mean or the variance are not estimated correctly, the resulting distribution will show a distinct deviation from this uniform behavior.

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{../figures/CDFHistos_uniform} \includegraphics[scale=0.4]{../figures/CDFHistos_broad} \includegraphics[scale=0.4]{../figures/CDFHistos_narrow} \includegraphics[scale=0.4]{../figures/CDFHistos_over} \includegraphics[scale=0.4]{../figures/CDFHistos_under}
\caption{\label{fig:cdf_histos} Quantile distribution for different cases of estimating the full probability distribution compared to the expected distribution:
correct prediction (``uniform''), variance overestimated (``broad''), variance underestimated (``narrow''), mean overestimated (``over''), mean underestimated (``under'')}
\end{center}
\end{figure}

\subsubsection{Quantile Profile Plot}

We now refine the method from sec. \ref{sec:cdf_histo} by comparing (in the sense of higher or lower) the quantiles of the observed events, i.e. the sales records, with specified quantile values. In order to do so, we start again from the predicted values for the mean and variance from which a negative binomial distribution is constructed for each prediction, for example for the predicted demand for a single product sold on a single day in a given sales location. Each of these predicted negative binomial PDFs is then transformed to its CDF. Note that for simplicity, we always refer to the negative binomial model in this description, however, the general approach is valid for any PDF.

Then we compare the actual observed sales value (corrected for censored data if necessary) to different quantiles of the corresponding predicted distribution for each data record and average over a larger data sample. For example, if we wanted to check that the median of the distribution, corresponding to the $0.5$ quantile, is predicted correctly by the machine learning model, we would compare the quantile value $0.5$ to the ratio of quantile values of observed sales records being lower/higher than $0.5$. In other words, in case of the median, 50\% of the ex post observed target values should be observed below the median of the corresponding individual predicted PDF and 50\% above.

In order to judge whether the overall shape of the predicted distributions is predicted correctly, we repeat this procedure for a range of quantiles, for example $q = 0.1, 0.2, \ldots, 0.9$. However, we are free to choose which quantiles to look at, and in specific situations it might be advisable to look at the tails of the distribution in more detail, to make sure that even relatively rare events are estimated correctly by the machine learning algorithm, and add more quantiles for comparison in the region between, say $q = 0.95$ and $q = 0.99$. In the following, we call this method {\em quantile profile plot}.

\fig \ref{fig:quant_profiles} illustrates five different collections of quantile profile plots (each collection comparing to 7 specified quantile values), for separate sets of exemplary PDF estimations and observed data combinations. The dashed horizontal lines indicates the fraction we expect, i.e. the specified quantile value, if the predictions are correct. For example, for the median, the line at 0.5 indicates that  50 percent of all PDF prediction and observed data combinations in a given data set should fall above the line, and 50 percent should fall below the line. The observation of the number of samples, indicated with shaded circles, that do in fact fall above and below a particular line, then allows the evaluation of the accuracy of PDF estimations. In case that the PDFs are not estimated correctly, the fractions will deviate from their expected values and the corresponding profile plot allows to judge whether for example the tails of the predicted distribution describing rare events are particularly problematic.

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{../figures/invquant_profiles}
\caption{\label{fig:quant_profiles} Profile plots of quantiles comparing all predicted PDFs to the corresponding observed events. In the leftmost two columns of shaded circles illustrate the behavior if the estimate of the variance is biased, the center and center-right columns of shaded circles illustrate cases for which the mean estimation is biased and the far right column shows the expected behaviour if all predictions are correct.}
\end{center}
\end{figure}

In the same way as the method of filling all observed CDF values of the individual PDF predictions in a histogram and compare to a uniform distribution (described in sec. \ref{sec:cdf_histo}), quantile profile plots do not work on individual PDF predictions but require a statistical population. However, calculating the fractions of the quantile profile plots globally, i.e. over all samples in the data set, might not reveal certain deficiencies for a subset, e.g. a specific store or product group in the example of retail demand forecasts. Therefore, we combine the approach described above with the method of profile plots, where the quantity on the x-axis of the quantile profile plot can be any variable of the data set at hand. Profile plots are akin to scatter plots and described in more detail in appendix \ref{sec:profile}.

In summary, this approach has two major benefits compared to the method discussed in sec. \ref{sec:cdf_histo}: First, by explicitly visualizing several different quantiles, the quantile profile plot reveals which part of the predicted PDF, such as the tails of the probability distribution, are particularly problematic. Second, by showing the dependency from the (arbitrary) variable on the x-axis, the quantile profile plot reveals deviations of the predicted PDF from the actuals for different parts, e.g. specific categories, of that variable. Two examples for this can be found in figures \ref{fig:invquant_mean} and \ref{fig:invquant_dayofweek} in the next section.

\subsection{Quantitative Evaluation of PDF Predictions}
\label{sec:cdf_acc}

The methods discussed  so far allow a detailed qualitative evaluation of PDF predictions. However, in order to also quantify the quality of the PDF predictions, a metric is required that assesses the difference between the distributions. Therefore, we want to compare the CDF histogram of the predicted PDFs with the expected  uniform distribution and define a metric in the range between 0 and 1, such that the metric takes the value of 1 when both distributions agree perfectly. Several approaches that measure the difference between two probability distributions are suggested in the literature such as  the first Wasserstein distance \cite{olkin1982}, the Kullback-Leibler divergence \cite{kullback1951}, and the Jensen-Shannon divergence  \cite{dagan1997}.

The first Wasserstein distance, also known as earth mover distance (EMD), represents a distance  defined between two probability distributions on a given metric space, and for our purposes here can defined by:
\begin{equation}
\text{EMD}(P, Q) = 2 \cdot \frac{\sum_{k=1}^N |F_P(x_k) - F_Q(x_k)|}{N},
\end{equation}
where $F_P(X)$ and $F_Q(X)$ are the CDFs of the two PDFs $P(X)$ and $Q(X)$, respectively, and $x_k$ denotes the average value of $X$ in bin $k$, with $X$ being divided in $N$ bins. Compared to the other metrics given below, the first Wasserstein or earth mover distance has the additional benefit, that it takes a specific metric space into account, meaning that it depends on the distance of potential deviations.

The Kullback-Leibler divergence is a measure of how one probability distribution diverges from a second expected probability distribution. For PDFs $P(X)$ and $Q(X)$, again with $X$ being divided in $N$ bins $k$, the Kullback-Leibler divergence from Q to P is defined as:
\begin{equation}
D_{\text{KL}}(P \parallel Q) = \sum _{k=1}^N P(x_k) \log \left({\frac{P(x_k)}{Q(x_k)}}\right),
\end{equation}
where the logarithm can be either base-2 or the natural logarithm.

The Jensen-Shannon divergence (JSD) can be seen as a symmetrized and smoothed version of the Kullback-Leibler divergence:
\begin{equation}
D_{\text{JSD}}(P \parallel Q) = 0.5  \cdot (D_{\text{KL}}(P \parallel M) + D_{\text{KL}}(P \parallel M)),
\end{equation}
where $M = 0.5  \cdot (P + Q)$.


\section{Example: Demand Forecasting}
\label{sec:example}

We use data from a Kaggle online competition about estimating unit sales of Walmart retail goods \cite{kaggle_data} to demonstrate forecasting of demand for different product-location-date combinations in form of full individual PDFs by means of Cyclic Boosting and subsequent qualitative and quantitative evaluation of the PDF predictions. The fields identifying a unique sample of the data set are store (\texttt{store\_id}) and item identifiers (\texttt{item\_id}) as well as date. The target $y$ which we need to predict is the number of sales of a given product in a given store on a specific day, denoted by \texttt{sales}. In the following, we use data from 2013-01-01 to 2016-05-22, that describe the sales of 100 different products (\texttt{FOODS\_3\_500}, ..., \texttt{FOODS\_3\_599}) of the department \texttt{FOODS\_3} in 10 stores. All data before 2016 are used as the training data and the data from 2016 are used as an independent test or validation set. Besides the fields used to identify an individual sales record and the corresponding observed sales value, namely \texttt{item\_id}, \texttt{store\_id}, \texttt{date}, \texttt{sales}, we also use the fields \texttt{event\_name\_1}, \texttt{event\_type\_1}, \texttt{snap\_CA}, \texttt{snap\_TX}, \texttt{snap\_WI}, \texttt{sell\_price}, and \texttt{list\_price} to build features for our machine learning models.

\subsection{Mean Estimation}
\label{sec:example_mean}

The first model, based on the Cyclic Boosting approach described above, is used to predict the mean of an assumed negative binomial distribution, and we use the following variables as features: categorical variables for \texttt{store\_id} and \texttt{item\_id}, several derived variables that are constructed from the time-series of the sales records describing trend and seasonality (days since beginning of 2013 as linear trend as well as day of week, day of year, month, and week of month), time windows around the events given in the data set (7 days before until 3 days after for Christmas and Easter, and 3 days before until 1 day after for all other events like New Year or Thanksgiving), a flag denoting a promotion, and the ratio of reduced (\texttt{sell\_price}) and normal price (\texttt{list\_price}). We also include various two-dimensional combinations of these features. In these cases, one of the two dimensions is either \texttt{store\_id} or \texttt{item\_id}, allowing the machine learning model to learn characteristics of individual locations and products.

Unlike most state-of-the-art time series forecasting methods, we do not include lagged target information, for example via stacking of exponentially weighted moving average features, in our model. The reason for this is that including such information makes the learning of exogenous effects, e.g. promotions or events, much harder, as the model tends to rely mainly on temporal confounding. Omitting these kind of variables improves the capability of the machine learning model to learn causal dependencies, which in turn improves the explainability of the model as well as the quality of the forecasts for mid- to long-term predictions and rare events. In order to capture recent trends that are not reflected in the exogenous features of the model, we apply an individual residual correction on each of the predictions of the machine learning model, which accounts for deviations between the exponentially weighted moving average (with a recursive smoothing factor of $0.15$) of the predictions and targets of each product-location combination over the corresponding past. To reflect a realistic replenishment scenario, we use a prediction horizon of two days for our example here, i.e. a target lag of two days for the training.

\noindent
Just to give a rough verification for the accuracy of the mean predictions, we report the MAD and MSE averaged over the full used data set in the test period to be $1.65$ and $10.15$, respectively, while the average of the target, i.e. the sales, is $3.28$. \fig \ref{fig:mean_prediction} shows the time series of both predictions and sales summed over all 100 products and 10 stores during the test period.

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{../figures/mean_prediction}
\caption{\label{fig:mean_prediction} Time series of mean prediction and sales in test period summed over all products and stores.}
\end{center}
\end{figure}

\subsection{Variance Estimation}

The second model, also based on Cyclic Boosting, is used to estimate the variance of the negative binomial distribution. The data are split into training and test set as above and in addition the mean predictions for each individual product-location-date combination are fixed in the variance model as stated by \eqn \eqref{eqn:loss_likelihood}. This effectively means that the mean predictions are created in-sample for the training period using the fully trained and validated model for the mean discussed above.

In this model focusing on the variance, we use a different set of features: categorical variables for \texttt{store\_id} and \texttt{item\_id} and a flag denoting a promotion. An example for the resulting PDF and CDF predictions for item \texttt{FOODS\_3\_586} in store \texttt{WI\_1} on 2016-05-22 is shown in \fig \ref{fig:pdf_example}.

\begin{figure}
\begin{center}
\includegraphics[width=4cm]{../figures/pdf}
\includegraphics[width=4cm]{../figures/cdf}
\caption{\label{fig:pdf_example} Predicted PDF (left) and CDF (right) for a specific product-location-date combination.}
\end{center}
\end{figure}

\subsection{Evaluation of PDF Predictions}

\fig \ref{fig:cdf_demand} shows the histogram of CDF observations according to the method described in sec. \ref{sec:cdf_histo} for all product-location-day combinations in the test period. As benchmark, we compare the outcome of our negative binomial model to a simpler Poisson assumption, which has only a single model parameter, the mean. Using the same mean predictions for both negative binomial and Poisson model, the negative binomial PDF predictions are much closer to the uniform distribution, which we expect for optimal PDF predictions, than the Poisson PDF predictions, showing the effectiveness of our variance estimation. The only significant deviation of the negative binomial histogram from the uniform distribution can be found in the last bins of CDF values close to $1$. As can be seen when excluding samples with mean predictions lower than $1.0$, this deviation stems from slow-sellers, which are prone to overdispersion.

\begin{figure}
\begin{center}
\includegraphics[width=4cm]{../figures/cdf_truth_nbinom}
\includegraphics[width=4cm]{../figures/cdf_truth_poisson}
\includegraphics[width=4cm]{../figures/cdf_truth_nbinom_larger1}
\includegraphics[width=4cm]{../figures/cdf_truth_poisson_larger1}
\caption{\label{fig:cdf_demand} Histograms of ex post observed target CDF values of the corresponding individual PDF predictions (to be compared to a uniform distribution) for our negative binomial model (left) and a simpler Poisson model for comparison (right), using all product-location-day combinations in the test period (upper two plots). In order to show the effect of slow-sellers, the lower two plots exclude all samples with mean predictions lower than $1.0$.}
\end{center}
\end{figure}

\fig \ref{fig:invquant_mean} and \fig \ref{fig:invquant_dayofweek} show quantile profile plots for different variables on the x-axis, namely mean predictions and day of week, respectively. All product-location-day combinations in the test period are used to generate the statistics in the plots. It can be seen that the different weekdays (from Monday as 0 to Sunday as 6) show slightly different patterns and that there are larger deviations for higher mean predictions (with fewer samples though).

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{../figures/invquant_yhat_mean}
\caption{\label{fig:invquant_mean} Quantile profile plot for mean predictions on the x-axis aggregated over all product-location-day combinations in the test period.}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{../figures/invquant_dayofweek}
\caption{\label{fig:invquant_dayofweek} Quantile profile plot for the different days of week on the x-axis (from Monday to Sunday) aggregated over all product-location-day combinations in the test period.}
\end{center}
\end{figure}

The quantitative results for the CDF accuracies of our PDF predictions for the different metrics described in sec. \ref{sec:cdf_acc}, calculated over all product-location-day combinations in the test period, can be found in \tab \ref{tab:cdf:acc}. As benchmark, we compare these to the results of a Poisson model assumption needing only the mean as single parameter. Our negative binomial PDF predictions show a significant improvement over the simpler Poisson model, which uses the same mean predictions, for each of the metrics.

\begin{table}[h!]
\begin{center}
\caption{CDF accuracies for negative binomial and Poisson PDF predictions for different metrics calculated over all product-location-day combinations in the test period.}
\label{tab:cdf:acc}
\begin{tabular}{c|c|c}
 & \textbf{NBD} & \textbf{Poisson} \\
\hline
\textbf{EMD} & 0.965 & 0.852 \\
\textbf{KL\_2} & 0.995 & 0.933 \\
\textbf{KL\_e} & 0.997 & 0.954 \\
\textbf{JSD\_2} & 0.999 & 0.982 \\
\textbf{JSD\_e} & 0.999 & 0.987
\end{tabular}
\end{center}
\end{table}


\section{Conclusion}

We have shown how to use two subsequent machine learning models for mean and variance estimation together with a negative binomial model assumption to come up with individual PDF predictions. This setup is especially useful for retail demand forecasting, where the true demand follows a negative binomial distribution quite closely. Compared to a model-free approach like quantile regression, the distributional assumption therefore drastically reduces the uncertainty of the resulting predictions, which are, by using Cyclic Boosting as underlying machine learning algorithm, fully explainable on the individual level.

Furthermore, we have presented new qualitative and quantitative methods for evaluating predictions in form of full PDFs. For qualitative evaluation, it is important to check the full PDF, especially its tails that are often crucial for subsequent decision making, as well as investigate aggregations of individual PDFs (to reduce the uncertainty of the evaluation) dependent on specific variables, and we have proposed a novel profile approach called quantile profile plot for this. For quantitative evaluation, it is always desirable to have a single number to compare different models in terms of accuracy, and we have suggested to use the deviance between the CDF histogram of the predicted PDFs and the uniform distribution to compare PDF accuracies.


\bibliography{paper}
\bibliographystyle{ieeetr}

%%
%% Appendices
%%
\appendix

\section{Profile Histograms}
\label{sec:profile}

In many cases, scatter plots are used to study the behaviour of two distributions or sets of data points visually. However, even for moderate amount of data, this approach becomes quickly difficult. To illustrate this, a sample of $(x,y)$ data points was obtained in the following way: The distribution of $x$ values was obtained by generating 5,000 samples of Gaussian distributed random numbers $X \sim {\cal N}(0.0,2.0)$ and the $y$ values are obtained via $Y \sim X +  {\cal N}(2.0,1.5)$. \fig \ref{fig:scatter} shows the marginal distributions for $x$ and $y$ as well as a scatter plot of $x$ vs. $y$.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../figures/marginal} \includegraphics[scale=0.5]{../figures/scatter}
\caption{\label{fig:scatter} Marginal distribution and scatter plot of variables $X$ and $Y$.}
\end{center}
\end{figure}

Although the simple linear correlation between $X$ and $Y$ is apparent in the scatter plot, finer details are not visible and it is easy to imagine that a more complex relationship is difficult to discern. Profile histograms are specifically designed to address this shortcoming. Intuitively, profile histograms are a one-dimensional representation of the two-dimensional scatter plot and are obtained  in the following way: The variable on the $x$ axis is discretized into a suitable range of bins. The exact choice of binning depends on the problem at hand. One can for example choose equidistant bins in the range of the $x$ axis or non-equidistant bins such that each bin contains the same number of observations. Then within each bin of the variable $X$, the a location and dispersion metric is calculated for the variable $Y$. This means that the bin-borders on the $X$ axis are used as constraints on the variable $Y$ and with these conditions applied, for example the sample mean of the selected $y$ values as well as the standard deviation are calculated. These location and dispersion metrics in each bin of $X$ are used to illustrate the behaviour of the variable $Y$ as the values of the variable $X$ change from bin to bin. The resulting profile histogram is shown in \fig \ref{fig:profile}. This one-dimensional representation allows to understand even a complex relationship between two variables visually. Note that due to few data points at the edges of the distributions the profile histogram is expected to show visual artifacts in the corresponding regions.

 \begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../figures/profile}
\caption{\label{fig:profile} Profile histogram of variables $X$ and $Y$.}
\end{center}
\end{figure}


\end{document}
